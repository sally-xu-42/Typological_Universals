#!/bin/bash

#SBATCH -n 1
#SBATCH --cpus-per-task=16
#SBATCH --mem-per-cpu=16384
#SBATCH --gpus=rtx_2080_ti:1      ### original 8, can increase if we have more GPUs in the future
#SBATCH --gres=gpumem:11264m
#SBATCH --time=118:00:00
#SBATCH --open-mode=truncate
#SBATCH --mail-type=END,FAIL

#! Work directory (i.e. where the job will run):
workdir="$SLURM_SUBMIT_DIR"  # The value of SLURM_SUBMIT_DIR sets workdir to the directory
                             # in which sbatch is run (always run from root).

module purge
module load eth_proxy gcc/8.2.0 python_gpu/3.9.9
source $workdir/env2/bin/activate


#! Run options for the application:
prefix=""
application="src/learn/train_model.sh"
options=""


cd $workdir
echo -e "Changed directory to `pwd`.\n"

echo -e "JobID: $SLURM_JOB_ID\n======"
echo "Time: `date`"
echo "Running on master node: `hostname`"
echo "Current directory: `pwd`"


CMD="$prefix $workdir/$application $options"
echo -e "\nExecuting command:\n==================\n$CMD\n"
eval $CMD