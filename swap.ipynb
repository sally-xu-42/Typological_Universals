{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in ./.venv/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: emoji in ./.venv/lib/python3.9/site-packages (from stanza) (2.8.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from stanza) (1.25.2)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.9/site-packages (from stanza) (4.24.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.9/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in ./.venv/lib/python3.9/site-packages (from stanza) (2.0.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from stanza) (4.66.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (4.7.1)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->stanza) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->stanza) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.9/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 15.4MB/s]                    \n",
      "2023-08-31 01:45:49 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-08-31 01:45:50 INFO: File exists: /Users/sally/stanza_resources/en/default.zip\n",
      "2023-08-31 01:45:54 INFO: Finished downloading models and saved to /Users/sally/stanza_resources.\n",
      "2023-08-31 01:45:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 28.1MB/s]                    \n",
      "2023-08-31 01:45:56 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-08-31 01:45:56 INFO: Using device: cpu\n",
      "2023-08-31 01:45:56 INFO: Loading: tokenize\n",
      "2023-08-31 01:45:56 INFO: Loading: pos\n",
      "2023-08-31 01:45:57 INFO: Loading: lemma\n",
      "2023-08-31 01:45:57 INFO: Loading: constituency\n",
      "2023-08-31 01:45:57 INFO: Loading: depparse\n",
      "2023-08-31 01:45:57 INFO: Loading: sentiment\n",
      "2023-08-31 01:45:58 INFO: Loading: ner\n",
      "2023-08-31 01:45:58 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "%pip install stanza\n",
    "import stanza\n",
    "import copy\n",
    "\n",
    "# Load the English pipeline\n",
    "stanza.download('en')  # Download the English model\n",
    "nlp = stanza.Pipeline('en')  # Initialize the English pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'text': 'Bill',\n",
       "  'lemma': 'Bill',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 2,\n",
       "  'deprel': 'nsubj',\n",
       "  'start_char': 0,\n",
       "  'end_char': 4,\n",
       "  'ner': 'S-PERSON',\n",
       "  'multi_ner': ('S-PERSON',)},\n",
       " {'id': 2,\n",
       "  'text': 'seems',\n",
       "  'lemma': 'seem',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VBZ',\n",
       "  'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "  'head': 0,\n",
       "  'deprel': 'root',\n",
       "  'start_char': 5,\n",
       "  'end_char': 10,\n",
       "  'ner': 'O',\n",
       "  'multi_ner': ('O',)},\n",
       " {'id': 3,\n",
       "  'text': 'honest',\n",
       "  'lemma': 'honest',\n",
       "  'upos': 'ADJ',\n",
       "  'xpos': 'JJ',\n",
       "  'feats': 'Degree=Pos',\n",
       "  'head': 2,\n",
       "  'deprel': 'xcomp',\n",
       "  'start_char': 11,\n",
       "  'end_char': 17,\n",
       "  'ner': 'O',\n",
       "  'multi_ner': ('O',)}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Bill seems honest\")\n",
    "sentence = doc.sentences[0].to_dict()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = copy.deepcopy(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCoarse(x):\n",
    "    if \":\" in x:\n",
    "        return x[: x.index(\":\")]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_children(sentence):\n",
    "    \"\"\" Coarsify all the dependent relations, track all children \"\"\"\n",
    "    for line in sentence:\n",
    "        # make the dependency relation label coarse (ignore stuff after colon)\n",
    "        line[\"coarse_dep\"] = makeCoarse(line[\"deprel\"])\n",
    "\n",
    "        # identify the root, and skip to next word\n",
    "        if line[\"coarse_dep\"] == \"root\":\n",
    "            root = line[\"id\"]\n",
    "            continue\n",
    "\n",
    "        if line[\"coarse_dep\"].startswith(\"punct\"):\n",
    "            continue\n",
    "\n",
    "        headIndex = line[\"head\"] - 1\n",
    "        sentence[headIndex][\"children\"] = sentence[headIndex].get(\"children\", []) + [line[\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'text': 'Bill',\n",
       "  'lemma': 'Bill',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 2,\n",
       "  'deprel': 'nsubj',\n",
       "  'start_char': 0,\n",
       "  'end_char': 4,\n",
       "  'ner': 'S-PERSON',\n",
       "  'multi_ner': ('S-PERSON',),\n",
       "  'coarse_dep': 'nsubj'},\n",
       " {'id': 2,\n",
       "  'text': 'seems',\n",
       "  'lemma': 'seem',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VBZ',\n",
       "  'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "  'head': 0,\n",
       "  'deprel': 'root',\n",
       "  'start_char': 5,\n",
       "  'end_char': 10,\n",
       "  'ner': 'O',\n",
       "  'multi_ner': ('O',),\n",
       "  'children': [1, 3],\n",
       "  'coarse_dep': 'root'},\n",
       " {'id': 3,\n",
       "  'text': 'honest',\n",
       "  'lemma': 'honest',\n",
       "  'upos': 'ADJ',\n",
       "  'xpos': 'JJ',\n",
       "  'feats': 'Degree=Pos',\n",
       "  'head': 2,\n",
       "  'deprel': 'xcomp',\n",
       "  'start_char': 11,\n",
       "  'end_char': 17,\n",
       "  'ner': 'O',\n",
       "  'multi_ner': ('O',),\n",
       "  'coarse_dep': 'xcomp'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_children(sent)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_order(verb_idx, obj_idx, sentence, result):\n",
    "# Helper function for processing verb and object chunks\n",
    "    # verb_list = sentence[verb_idx]['children']\n",
    "    # obj_list = sentence[obj_idx]['children']\n",
    "    # verb_list = verb_list - obj_list\n",
    "    result[verb_idx], result[obj_idx] = sentence[obj_idx][\"id\"], sentence[verb_idx][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap(sentence, root):\n",
    "# DFS for swaping verb and object\n",
    "# TODO: edge cases: 1. multiple obj\n",
    "#                   2. went to school happily -> to school went happily\n",
    "    result = [i for i in range(1, len(sentence) + 1)]\n",
    "    stack = [root]\n",
    "    visited = set()\n",
    "\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            print(node) # print out index of the node being processed\n",
    "\n",
    "            if not sentence[node-1].get(\"children\", None):\n",
    "                continue\n",
    "            for c in sentence[node-1][\"children\"]:\n",
    "                if sentence[node-1]['upos'] == 'VERB' and sentence[c-1]['coarse_dep'] == 'obj':\n",
    "                    verb_idx, obj_idx = node - 1, c - 1\n",
    "                    swap_order(verb_idx, obj_idx, sentence, result)\n",
    "                if c not in visited:\n",
    "                    stack.append(c)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swap(sent, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
